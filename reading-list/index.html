<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Reading List | Rui Zhang&#39;s Blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Reading List是我在博士期间阅读和整理的文献内容。不定期更新。    Representation Learning 表示学习I. Theory Basis 理论基础 Harris Z S. Distributional structure[J]. Word, 1954, 10(2-3): 146-162.  分布式表示的理论基础    II. Word Embedding 词向量Pa">
<meta property="og:type" content="website">
<meta property="og:title" content="Reading List">
<meta property="og:url" content="http://ruizhang1993.github.io/reading-list/index.html">
<meta property="og:site_name" content="Rui Zhang&#39;s Blogs">
<meta property="og:description" content="Reading List是我在博士期间阅读和整理的文献内容。不定期更新。    Representation Learning 表示学习I. Theory Basis 理论基础 Harris Z S. Distributional structure[J]. Word, 1954, 10(2-3): 146-162.  分布式表示的理论基础    II. Word Embedding 词向量Pa">
<meta property="og:updated_time" content="2017-08-25T03:28:08.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reading List">
<meta name="twitter:description" content="Reading List是我在博士期间阅读和整理的文献内容。不定期更新。    Representation Learning 表示学习I. Theory Basis 理论基础 Harris Z S. Distributional structure[J]. Word, 1954, 10(2-3): 146-162.  分布式表示的理论基础    II. Word Embedding 词向量Pa">
  
    <link rel="alternate" href="/atom.xml" title="Rui Zhang&#39;s Blogs" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
          <li><a class="active"
                 href="/reading-list/">Reading List</a></li>
        
          <li><a class=""
                 href="/about-me/">About Me</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <!--<h1 class="blog-title">Rui Zhang&#39;s Blogs</h1>-->
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          <article id="page-" class="article article-type-page" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      Reading List
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/reading-list/index.html" class="article-date"><time datetime="2017-08-25T03:28:08.000Z" itemprop="datePublished">2017-08-25</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>Reading List是我在博士期间阅读和整理的文献内容。不定期更新。  </p>
</blockquote>
<h1 id="Representation-Learning-表示学习"><a href="#Representation-Learning-表示学习" class="headerlink" title="Representation Learning 表示学习"></a><strong>Representation Learning 表示学习</strong></h1><h2 id="I-Theory-Basis-理论基础"><a href="#I-Theory-Basis-理论基础" class="headerlink" title="I. Theory Basis 理论基础"></a>I. Theory Basis 理论基础</h2><ul>
<li>Harris Z S. <strong>Distributional structure</strong>[J]. Word, 1954, 10(2-3): 146-162.<br>  <em>分布式表示的理论基础</em>  </li>
</ul>
<h2 id="II-Word-Embedding-词向量"><a href="#II-Word-Embedding-词向量" class="headerlink" title="II. Word Embedding 词向量"></a>II. Word Embedding 词向量</h2><h3 id="Papers"><a href="#Papers" class="headerlink" title="Papers"></a>Papers</h3><ul>
<li>孙飞,郭嘉丰,兰艳艳,徐君,程学旗. <strong>分布式单词表示综述</strong>[J]. 计算机学报,2016,:1-22.<br>  <em>关于词向量表示的中文综述。</em></li>
<li>Bengio Y, Ducharme R, Vincent P, et al. <strong>A neural probabilistic language model</strong>[J]. journal of machine learning research, 2003, 3(Feb): 1137-1155.<br>  <em>最经典的神经网络语言模型</em></li>
<li>Collobert R, Weston J, Bottou L, et al. <strong>Natural language processing (almost) from scratch</strong>[J]. Journal of Machine Learning Research, 2011, 12(Aug): 2493-2537.<br>  <em>C&amp;W模型，利用前文和后文的上下文窗口，采用排序损失函数对单词序列进行打分</em>  </li>
<li>Mikolov T, Kopecky J, Burget L, et al. <strong>Neural network based language models for highly inflective languages</strong>[C]//2009 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2009: 4725-4728.<br>  <em>比较原始的模型，利用简单的神经网络建模，根据一定窗口内的词汇预测下一个单词</em>  </li>
<li>Mikolov T, Kombrink S, Burget L, et al. <strong>Extensions of recurrent neural network language model</strong>[C]//2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2011: 5528-5531.<br>  <em>利用BPTT和对词语进行分类来加速模型的训练，其中类别是用词语出现的频繁度来衡量的</em>  </li>
<li>Kombrink S, Mikolov T, Karafiát M, et al. <strong>Recurrent Neural Network Based Language Modeling in Meeting Recognition</strong>[C]//INTERSPEECH. 2011, 11: 2877-2880.<br>  <em>RNNLM，运用循环神经网络根据过去的输入预测下一个输入，并应用于语音识别</em>   </li>
<li>Mikolov T, Deoras A, Povey D, et al. <strong>Strategies for training large scale neural network language models</strong>[C]//Automatic Speech Recognition and Understanding (ASRU), 2011 IEEE Workshop on. IEEE, 2011: 196-201.<br>  <em>利用最大熵模型来降低神经语言模型的训练复杂度</em>  </li>
<li>Mikolov T. <strong>Statistical language models based on neural networks</strong>[J]. Presentation at Google, Mountain View, 2nd April, 2012.<br>  <em>Google Scholar上可以下载到的PPT</em>  </li>
<li>Mikolov T, Sutskever I, Chen K, et al. <strong>Distributed representations of words and phrases and their compositionality</strong>[C]//Advances in neural information processing systems. 2013: 3111-3119.<br>  <em>利用层次Softmax，负采样和频繁词subsample来提高词向量训练效果</em></li>
<li>Mikolov T, Chen K, Corrado G, et al. <strong>Efficient estimation of word representations in vector space</strong>[J]. arXiv preprint arXiv:1301.3781, 2013.<br>  <em>对当时已有的神经网络词向量生成模型进行比对</em>  </li>
</ul>
<h3 id="Blogs-Code-amp-Data"><a href="#Blogs-Code-amp-Data" class="headerlink" title="Blogs, Code &amp; Data"></a>Blogs, Code &amp; Data</h3><ul>
<li>Word2Vec Tutorial - The Skip-Gram Model<br><a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" target="_blank" rel="external">http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/</a>  </li>
<li>Alex Minnaar’s Tutorials<ol>
<li><a href="http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf" target="_blank" rel="external">Part I - The Skip-Gram Model</a>  </li>
<li><a href="http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf" target="_blank" rel="external">Part II - Continuous Bag-of-Words Model</a>   </li>
</ol>
</li>
<li>word2vec 中的数学原理详解<ol>
<li><a href="http://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="external">目录和前言</a>  </li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969635" target="_blank" rel="external">预备知识</a>  </li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969817" target="_blank" rel="external">背景知识</a>  </li>
<li><a href="http://blog.csdn.net/itplus/article/details/37969979" target="_blank" rel="external">基于 Hierarchical Softmax 的模型</a>  </li>
<li><a href="http://blog.csdn.net/itplus/article/details/37998797" target="_blank" rel="external">基于 Negative Sampling 的模型</a>  </li>
<li><a href="http://blog.csdn.net/itplus/article/details/37999613" target="_blank" rel="external">若干源码细节</a>  </li>
</ol>
</li>
<li>Deep Learning实战之word2vec<ol>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_699" target="_blank" rel="external">什么是word2vec?</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_838" target="_blank" rel="external">快速入门</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_1027" target="_blank" rel="external">作者八卦</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_1077" target="_blank" rel="external">背景知识</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_2413" target="_blank" rel="external">模型</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_3269" target="_blank" rel="external">Tricks</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_3974" target="_blank" rel="external">分布式实现</a>  </li>
<li><a href="http://techblog.youdao.com/?p=915#LinkTarget_4109" target="_blank" rel="external">总结</a>  </li>
</ol>
</li>
</ul>
<h2 id="III-Sentiment-Specific-Word-Embeddings-情感词向量"><a href="#III-Sentiment-Specific-Word-Embeddings-情感词向量" class="headerlink" title="III. Sentiment-Specific Word Embeddings 情感词向量"></a>III. Sentiment-Specific Word Embeddings 情感词向量</h2><h3 id="Papers-1"><a href="#Papers-1" class="headerlink" title="Papers"></a>Papers</h3><ul>
<li>Maas A L, Daly R E, Pham P T, et al. <strong>Learning word vectors for sentiment analysis</strong>[C]//Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies-Volume 1. Association for Computational Linguistics, 2011: 142-150.<br>  <em>基于话题概率模型</em>  </li>
<li>Labutov I, Lipson H. <strong>Re-embedding words</strong>[C]//ACL (2). 2013: 489-493.<br>  <em>将情感值作为正则项结合逻辑回归训练词向量</em>  </li>
<li>Tang D, Wei F, Yang N, et al. <strong>Learning Sentiment-Specific Word Embedding for Twitter Sentiment Classification</strong>[C]//ACL (1). 2014: 1555-1565.<br>  <em>基于C&amp;W模型的情感词向量训练</em>    </li>
<li>Zhang Z, Lan M. <strong>Learning sentiment-inherent word embedding for word-level and sentence-level sentiment analysis</strong>[C]//2015 International Conference on Asian Language Processing (IALP). IEEE, 2015: 94-97.<br>  <em>基于Skip-Gram的情感词向量训练</em>  </li>
<li>Tang D, Wei F, Qin B, et al. <strong>Building Large-Scale Twitter-Specific Sentiment Lexicon: A Representation Learning Approach</strong>[C]//COLING. 2014: 172-182.<br>  <em>利用Twitter文本基于Skip-Gram构建大规模情感词典</em>  </li>
</ul>
<h3 id="To-Read-List"><a href="#To-Read-List" class="headerlink" title="To-Read List"></a>To-Read List</h3><ul>
<li>Du H, Xu X, Cheng X, et al. <strong>Aspect-Specific Sentimental Word Embedding for Sentiment Analysis of Online Reviews</strong>[C]//Proceedings of the 25th International Conference Companion on World Wide Web. International World Wide Web Conferences Steering Committee, 2016: 29-30.</li>
<li>Ren Y, Wang R, Ji D. <strong>A topic-enhanced word embedding for Twitter sentiment classification</strong>[J]. Information Sciences, 2016, 369: 188-198.</li>
<li>Lan M, Zhang Z, Lu Y, et al. <strong>Three Convolutional Neural Network-based models for learning Sentiment Word Vectors towards sentiment analysis</strong>[C]//Neural Networks (IJCNN), 2016 International Joint Conference on. IEEE, 2016: 3172-3179.</li>
<li>Xiong S. <strong>Improving Twitter Sentiment Classification via Multi-Level Sentiment-Enriched Word Embeddings</strong>[J]. arXiv preprint arXiv:1611.00126, 2016.</li>
<li>Giatsoglou M, Vozalis M G, Diamantaras K, et al. <strong>Sentiment analysis leveraging emotions and word embeddings</strong>[J]. Expert Systems with Applications, 2017, 69: 214-224.</li>
<li>Tang D, Wei F, Qin B, et al. <strong>Sentiment Embeddings with Applications to Sentiment Analysis</strong>[J]. IEEE Transactions on Knowledge and Data Engineering, 2016, 28(2): 496-509.</li>
</ul>
<h2 id="IV-Phrase-Sentence-Embeddings-短语-句子向量表示"><a href="#IV-Phrase-Sentence-Embeddings-短语-句子向量表示" class="headerlink" title="IV. Phrase/Sentence Embeddings 短语/句子向量表示"></a>IV. Phrase/Sentence Embeddings 短语/句子向量表示</h2><h3 id="To-Read-List-1"><a href="#To-Read-List-1" class="headerlink" title="To-Read List"></a>To-Read List</h3><ul>
<li>Collobert R, Weston J, Bottou L, et al. <strong>Natural language processing (almost) from scratch</strong>[J]. Journal of Machine Learning Research, 2011, 12(Aug): 2493-2537.</li>
<li>Kalchbrenner N, Grefenstette E, Blunsom P. <strong>A convolutional neural network for modelling sentences</strong>[J]. arXiv preprint arXiv:1404.2188, 2014.</li>
<li>Kim Y. <strong>Convolutional neural networks for sentence classification</strong>[J]. arXiv preprint arXiv:1408.5882, 2014.</li>
<li>Xin Wang, Yuanchao Liu, Chengjie Sun, Baoxun Wang, and Xiaolong Wang. <strong>Predicting polarities of tweets by composing word embeddings with long short-term memory</strong>. 2015. In Proceedings of ACL 2015</li>
<li>Zhiyang Teng, Duy Tin Vo and Yue Zhang. <strong>Context-Sensitive Lexicon Features for Neural Sentiment Analysis</strong>. In Proceeddings of EMNLP 2016</li>
<li>Han Zhao, Zhengdong Lu, and Pascal Poupart. <strong>Self-adaptive hierarchical sentence model</strong>. In Proceedings of IJCAI2015.</li>
<li>Yin W, Schütze H. <strong>Multichannel variable-size convolution for sentence classification</strong>[J]. arXiv preprint arXiv:1603.04513, 2016.</li>
<li>Zhang Y, Roller S, Wallace B. <strong>Mgnc-cnn: A simple approach to exploiting multiple word embeddings for sentence classification</strong>[J]. arXiv preprint arXiv:1603.00968, 2016.</li>
<li>dos Santos C N, Gatti M. <strong>Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts</strong>[C]//COLING. 2014: 69-78.</li>
<li>Zhang, Rui and Lee, Honglak and Radev, Dragomir R. <strong>Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents</strong>. In Proceedings of NAACL2016</li>
</ul>
<h1 id="Neural-Network-神经网络"><a href="#Neural-Network-神经网络" class="headerlink" title="Neural Network 神经网络"></a><strong>Neural Network 神经网络</strong></h1><h2 id="I-Basic-Neural-Networks-基本神经网络"><a href="#I-Basic-Neural-Networks-基本神经网络" class="headerlink" title="I. Basic Neural Networks 基本神经网络"></a>I. Basic Neural Networks 基本神经网络</h2><h3 id="Blogs-Code-amp-Data-1"><a href="#Blogs-Code-amp-Data-1" class="headerlink" title="Blogs, Code &amp; Data"></a>Blogs, Code &amp; Data</h3><ul>
<li>WILDML<br>  <a href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/" target="_blank" rel="external">IMPLEMENTING A NEURAL NETWORK FROM SCRATCH IN PYTHON</a></li>
</ul>
<h2 id="II-Recurrent-Neural-Networks-循环神经网络"><a href="#II-Recurrent-Neural-Networks-循环神经网络" class="headerlink" title="II. Recurrent Neural Networks 循环神经网络"></a>II. Recurrent Neural Networks 循环神经网络</h2><h3 id="Blogs-Code-amp-Data-2"><a href="#Blogs-Code-amp-Data-2" class="headerlink" title="Blogs, Code &amp; Data"></a>Blogs, Code &amp; Data</h3><ul>
<li>LSTM简介以及数学推导<br>  <a href="http://blog.csdn.net/a635661820/article/details/45390671" target="_blank" rel="external">LSTM简介以及数学推导(FULL BPTT)</a></li>
<li>WILDML - RECURRENT NEURAL NETWORKS TUTORIAL<ol>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/" target="_blank" rel="external">Introduction to RNNs</a></li>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/" target="_blank" rel="external">Implementing a RNN using Python and Theano</a></li>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/" target="_blank" rel="external">Understanding the Backpropagation Through Time (BPTT) algorithm and the vanishing gradient problem</a></li>
<li><a href="http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/" target="_blank" rel="external">Implementing a GRU/LSTM RNN</a>  </li>
</ol>
</li>
</ul>
<h2 id="III-Attention-注意力机制"><a href="#III-Attention-注意力机制" class="headerlink" title="III. Attention 注意力机制"></a>III. Attention 注意力机制</h2><h3 id="Blogs-Code-amp-Data-3"><a href="#Blogs-Code-amp-Data-3" class="headerlink" title="Blogs, Code &amp; Data"></a>Blogs, Code &amp; Data</h3><ul>
<li>WILDML<br>  <a href="http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/" target="_blank" rel="external">ATTENTION AND MEMORY IN DEEP LEARNING AND NLP</a></li>
<li>自然语言处理中的Attention Model：是什么及为什么<br>  <a href="http://blog.csdn.net/malefactor/article/details/50550211#0-tsina-1-96365-397232819ff9a47a7b7e80a40613cfe1" target="_blank" rel="external">自然语言处理中的Attention Model：是什么及为什么</a></li>
<li>注意力机制（Attention Mechanism）在自然语言处理中的应用<br>  <a href="https://www.52ml.net/20093.html" target="_blank" rel="external">注意力机制（Attention Mechanism）在自然语言处理中的应用</a>  </li>
</ul>
<h1 id="Sentiment-Analysis-情感分析"><a href="#Sentiment-Analysis-情感分析" class="headerlink" title="Sentiment Analysis 情感分析"></a><strong>Sentiment Analysis 情感分析</strong></h1><h2 id="I-Surveys-综述"><a href="#I-Surveys-综述" class="headerlink" title="I. Surveys 综述"></a>I. Surveys 综述</h2><h3 id="Papers-2"><a href="#Papers-2" class="headerlink" title="Papers"></a>Papers</h3><ul>
<li>秦兵,唐都钰,袁建华. <strong>文本情感分析：让机器读懂人类情感</strong>.中国人工智能学会通讯，2016年第6卷第6期<br>  <em>中文综述</em></li>
<li>Tang D, Qin B, Liu T. <strong>Deep learning for sentiment analysis: successful approaches and future challenges</strong>[J]. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2015, 5(6): 292-303.<br>  <em>英文综述</em>  </li>
</ul>
<h2 id="II-Sentiment-Classification-情感分类"><a href="#II-Sentiment-Classification-情感分类" class="headerlink" title="II. Sentiment Classification 情感分类"></a>II. Sentiment Classification 情感分类</h2><h3 id="Papers-3"><a href="#Papers-3" class="headerlink" title="Papers"></a>Papers</h3><ul>
<li>Xiao Z, Liang P J. <strong>Chinese Sentiment Analysis Using Bidirectional LSTM with Word Embedding</strong>[C]//International Conference on Cloud Computing and Security. Springer International Publishing, 2016: 601-610.<br>  <em>基于双向LSTM的模型，针对中文文本</em></li>
<li>Tang D, Qin B, Liu T. <strong>Learning semantic representations of users and products for document level sentiment classification</strong>[C]//Proc. ACL. 2015.<br>  <em>融合用户矩阵和产品矩阵信息的篇章级情感分类</em></li>
<li>Chen H, Sun M, Tu C, et al. <strong>Neural Sentiment Classification with User and Product Attention</strong>[J].<br>  <em>基于用户和产品Attention的情感分类</em></li>
<li>Tang D, Qin B, Liu T. <strong>Document modeling with gated recurrent neural network for sentiment classification</strong>[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015: 1422-1432.<br>  <em>基于Gated-RNN的篇章级情感分类</em></li>
<li>Tang D, Qin B, Feng X, et al. <strong>Target-Dependent Sentiment Classification with Long Short Term Memory</strong>[J]. arXiv preprint arXiv:1512.01100, 2015.<br>  <em>利用双向LSTM建模的实体级情感分类</em></li>
</ul>
<h1 id="Conversation-Generation-对话生成"><a href="#Conversation-Generation-对话生成" class="headerlink" title="Conversation Generation 对话生成"></a><strong>Conversation Generation 对话生成</strong></h1><h2 id="I-Text-Generation-文本生成"><a href="#I-Text-Generation-文本生成" class="headerlink" title="I. Text Generation 文本生成"></a>I. Text Generation 文本生成</h2><h3 id="Papers-4"><a href="#Papers-4" class="headerlink" title="Papers"></a>Papers</h3><ul>
<li>Lopyrev K. <strong>Generating News Headlines with Recurrent Neural Networks</strong>[J]. arXiv preprint arXiv:1512.01712, 2015.<br>  <em>利用循环神经网络生成新闻标题</em></li>
</ul>
<h2 id="II-Conversation-Model-对话模型"><a href="#II-Conversation-Model-对话模型" class="headerlink" title="II. Conversation Model 对话模型"></a>II. Conversation Model 对话模型</h2><h3 id="Papers-5"><a href="#Papers-5" class="headerlink" title="Papers"></a>Papers</h3><p>[更新中…]</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://ruizhang1993.github.io/reading-list/index.html" data-id="cj6rc9apy001s0gupm1kb2q9g" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      

    </footer>
  </div>
  
    

  
</article>




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <font size="4"><b>张睿</b></font></br> - 华南理工大学 博士生 </br> - 研究方向： </br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自然语言处理、聊天机器人 </br> </br> <font size="4"><b>Rui Zhang</b></font></br> - Ph.D student @ SCUT </br> - Research domains: </br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NLP, chatbot

</div>


  


  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/CNN/">CNN</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Lex/">Lex</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Mac/">Mac</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/NLP/">NLP</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/PLY/">PLY</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Python/">Python</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/SMPSA2017/">SMPSA2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Windows7/">Windows7</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Yacc/">Yacc</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/cuda/">cuda</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/deep-learning/">deep learning</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/numpy/">numpy</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/python/">python</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/tensorflow/">tensorflow</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/xgboost/">xgboost</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/会议笔记/">会议笔记</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/数据挖掘/">数据挖掘</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/编译原理/">编译原理</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/论文笔记/">论文笔记</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Lex/" style="font-size: 10px;">Lex</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/NLP/" style="font-size: 16.67px;">NLP</a> <a href="/tags/PLY/" style="font-size: 10px;">PLY</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/SMPSA2017/" style="font-size: 10px;">SMPSA2017</a> <a href="/tags/Windows7/" style="font-size: 10px;">Windows7</a> <a href="/tags/Yacc/" style="font-size: 10px;">Yacc</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/deep-learning/" style="font-size: 20px;">deep learning</a> <a href="/tags/numpy/" style="font-size: 10px;">numpy</a> <a href="/tags/python/" style="font-size: 13.33px;">python</a> <a href="/tags/tensorflow/" style="font-size: 13.33px;">tensorflow</a> <a href="/tags/xgboost/" style="font-size: 10px;">xgboost</a> <a href="/tags/会议笔记/" style="font-size: 10px;">会议笔记</a> <a href="/tags/数据挖掘/" style="font-size: 10px;">数据挖掘</a> <a href="/tags/编译原理/" style="font-size: 10px;">编译原理</a> <a href="/tags/论文笔记/" style="font-size: 13.33px;">论文笔记</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/06/">June 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/05/">May 2017</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/04/">April 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/01/">January 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/11/">November 2016</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/07/">July 2016</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/05/">May 2016</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/04/">April 2016</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2017/06/11/SMP-SA2017-终身机器学习-Lifelong-Machine-Learning/">[SMP-SA2017] 终身机器学习LML</a>
        </li>
      
        <li>
          <a href="/2017/05/27/论文浅析-神经网络语言模型NNLM/">神经网络语言模型NNLM</a>
        </li>
      
        <li>
          <a href="/2017/05/21/卷积操作的简单理解/">卷积操作的简单理解</a>
        </li>
      
        <li>
          <a href="/2017/05/19/论文浅析-构建简单的神经网络对话生成模型/">构建简单的神经网络对话生成模型</a>
        </li>
      
        <li>
          <a href="/2017/05/01/Tensorflow之legacy-seq2seq模块初探-Tensorflow-r1-0-1/">Tensorflow之legacy_seq2seq模块初探(Tensorflow==r1.0.1)</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2017 RuiZhang1993<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



<script src="/js/script.js"></script>

</body>
</html>
