<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Tensorflow之legacy_seq2seq模块初探(Tensorflow==r1.0.1) | Rui Zhang&#39;s Blogs</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="本文对tensorflow中legacy_seq2seq模块中的部分重要类型和方法进行整理。文章基于tensorflow r1.0.1版本。">
<meta name="keywords" content="deep learning,tensorflow">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow之legacy_seq2seq模块初探(Tensorflow==r1.0.1)">
<meta property="og:url" content="http://ruizhang1993.github.io/2017/05/01/Tensorflow之legacy-seq2seq模块初探-Tensorflow-r1-0-1/index.html">
<meta property="og:site_name" content="Rui Zhang&#39;s Blogs">
<meta property="og:description" content="本文对tensorflow中legacy_seq2seq模块中的部分重要类型和方法进行整理。文章基于tensorflow r1.0.1版本。">
<meta property="og:updated_time" content="2017-08-24T03:30:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow之legacy_seq2seq模块初探(Tensorflow==r1.0.1)">
<meta name="twitter:description" content="本文对tensorflow中legacy_seq2seq模块中的部分重要类型和方法进行整理。文章基于tensorflow r1.0.1版本。">
  
    <link rel="alternate" href="/atom.xml" title="Rui Zhang&#39;s Blogs" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  <link rel="stylesheet" href="/css/styles.css">
  

</head>

<body>
  <nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/index.html">Home</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
          <li><a class=""
                 href="/tags/OPAD/">OPAD Project</a></li>
        
          <li><a class=""
                 href="/about/">About Me</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <!--<h1 class="blog-title">Rui Zhang&#39;s Blogs</h1>-->
  
</div>

    <div class="row">
        <div class="col-sm-8 blog-main">
          <article id="post-Tensorflow之legacy-seq2seq模块初探-Tensorflow-r1-0-1" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      Tensorflow之legacy_seq2seq模块初探(Tensorflow==r1.0.1)
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2017/05/01/Tensorflow之legacy-seq2seq模块初探-Tensorflow-r1-0-1/" class="article-date"><time datetime="2017-05-01T08:43:26.000Z" itemprop="datePublished">2017-05-01</time></a>
</div>

    
    

  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <blockquote>
<p>本文对tensorflow中legacy_seq2seq模块中的部分重要类型和方法进行整理。文章基于tensorflow r1.0.1版本。</p>
</blockquote>
<a id="more"></a>
<h2 id="attention-decoder"><a href="#attention-decoder" class="headerlink" title="attention_decoder"></a>attention_decoder</h2><p>这里的attention表示，在解码阶段，RNN将读取额外添加的张量attention_state中的信息，并关注该张量中的部分项。该实现基于<a href="http://arxiv.org/abs/1412.7449" target="_blank" rel="external">http://arxiv.org/abs/1412.7449</a>。推荐在复杂的seq2seq任务中使用该模型。</p>
<h4 id="输入参数"><a href="#输入参数" class="headerlink" title="输入参数"></a>输入参数</h4><p>1）<strong>decoder_inputs</strong>：A list of 2D Tensors [batch_size x input_size]. 一个2维tensor列表，作为decoder的输入；<br>2）<strong>initial_state</strong>：2D Tensor [batch_size x cell.state_size]. 一个2维Tensor，作为decoder的初始状态；<br>3）<strong>attention_states</strong>：3D Tensor [batch_size x attn_length x attn_size]. 一个3维Tensor，作为attention的状态；<br>4）<strong>cell</strong>：core_rnn_cell.RNNCell defining the cell function and size. 定义了RNNCell的类型；<br>5）<strong>output_size</strong>：Size of the output vectors; if None, we use cell.output_size. 定义了输出向量的长度，若为None则默认使用cell.output_size；<br>6）<strong>num_heads</strong>：Number of attention heads that read from attention_states. 从attention_states中读取attention heads的数量；<br>7）<strong>loop_function</strong>：If not None, this function will be applied to i-th output in order to generate i+1-th input, and decoder_inputs will be ignored, except for the first element (“GO” symbol). This can be used for decoding, but also for training to emulate <a href="http://arxiv.org/abs/1506.03099" target="_blank" rel="external">http://arxiv.org/abs/1506.03099</a>. Signature – loop_function(prev, i) = next<br>&emsp;&emsp;a. prev is a 2D Tensor of shape [batch_size x output_size],<br>&emsp;&emsp;b. i is an integer, the step number (when advanced control is needed),<br>&emsp;&emsp;c. next is a 2D Tensor of shape [batch_size x input_size].<br>&emsp;&emsp;如果loop_function不是None，就会将模型的上一时刻的输出作为下一时刻输入，并无视decoder_inputs输入；<br>8）<strong>dtype</strong>：The dtype to use for the RNN initial state (default: tf.float32). 数据类型；<br>9）<strong>scope</strong>：VariableScope for the created subgraph; default: “attention_decoder”.命名域，默认为”attention_decoder”；<br>10）<strong>initial_state_attention</strong>：If False (default), initial attentions are zero. If True, initialize the attentions from the initial state and attention states – useful when we wish to resume decoding from a previously stored decoder state and attention states. 默认状态下设置为False，将attention初始化为zero。若设置为True，则根据initial state和attention state进行初始化–可用于从先前存储的decoder state和attention state对模型进行恢复。</p>
<h4 id="返回参数"><a href="#返回参数" class="headerlink" title="返回参数"></a>返回参数</h4><p>返回一个(outputs, state)元组，其中：<br>1）<strong>outputs</strong>： A list of the same length as decoder_inputs of 2D Tensors of shape [batch_size x output_size]. These represent the generated outputs. Output i is computed from input i (which is either the i-th element of decoder_inputs or loop_function(output {i-1}, i)) as follows.<br>&emsp;&emsp;First, we run the cell on a combination of the input and previous attention masks:<br>&emsp;&emsp;&emsp;cell_output, new_state = cell(linear(input, prev_attn), prev_state).<br>&emsp;&emsp;Then, we calculate new attention masks:<br>&emsp;&emsp;&emsp;new_attn = softmax(V^T <em> tanh(W </em> attention_states + U <em> new_state))<br>&emsp;&emsp;and then we calculate the output:<br>&emsp;&emsp;&emsp;output = linear(cell_output, new_attn).<br>&emsp;&emsp;一个与decoder_inputs长度相同的2维Tensor的列表，其中Tensor形状为[batch_size x output_size]。表示生成得到的output。第i个output是根据第i个input根据下述方法计算得到：<br>&emsp;&emsp;首先组合当前输入和上一个attention mask结果，在此上运行cell：<br>&emsp;&emsp;&emsp;cell_output, new_state = cell(linear(input,pre_attn), prev_state)<br>&emsp;&emsp;随后计算新的attention mask：<br>&emsp;&emsp;&emsp;new_attn = softmax(V^T </em> tanh(W <em> attention_states + U </em> new_state))<br>&emsp;&emsp;然后计算输出output：<br>&emsp;&emsp;&emsp;output = linear(cell_output, new_attn)<br>2）<strong>state</strong>：The state of each decoder cell the final time-step. It is a 2D Tensor of shape [batch_size x cell.state_size].各个decoder在最后时间步的状态。形状为[batch_size, cell.state_size]</p>
<hr>
<h2 id="basic-rnn-seq2seq"><a href="#basic-rnn-seq2seq" class="headerlink" title="basic_rnn_seq2seq"></a>basic_rnn_seq2seq</h2><p>basic_rnn_seq2seq模型首先运行一个RNN将encoder_inputs编码成一个状态向量，随后运行使用encoder的最后一个状态初始化的decoder，将decoder_inputs进行解码。encoder和decoder使用相同的RNNcell类别，但不共享参数。  </p>
<h4 id="输入参数-1"><a href="#输入参数-1" class="headerlink" title="输入参数"></a>输入参数</h4><p>1）<strong>encoder_inputs</strong>: A list of 2D Tensors [batch_size x input_size]. 编码器输入，形状为[batch_size x input_size]<br>2）<strong>decoder_inputs</strong>: A list of 2D Tensors [batch_size x input_size]. 解码器输入，形状为[batch_size x input_size]<br>3）<strong>cell</strong>: core_rnn_cell.RNNCell defining the cell function and size. RNNcell类别<br>4）<strong>dtype</strong>: The dtype of the initial state of the RNN cell (default: tf.float32). 数据类型<br>5）<strong>scope</strong>: VariableScope for the created subgraph; default: “basic_rnn_seq2seq”. 命名域，默认为“basic_rnn_seq2seq”</p>
<h4 id="返回参数-1"><a href="#返回参数-1" class="headerlink" title="返回参数"></a>返回参数</h4><p>返回一个(outputs, state)元组，其中：<br>1）<strong>outputs</strong>: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x output_size] containing the generated outputs. 一组与decoder_inputs相同长度的2D-tensor。形状为[batch_size x output_size]<br>2）<strong>state</strong>: The state of each decoder cell in the final time-step. It is a 2D Tensor of shape [batch_size x cell.state_size]. 各个decoder在最后时间步的状态。形状为[batch_size, cell.state_size]</p>
<hr>
<h2 id="model-with-buckets"><a href="#model-with-buckets" class="headerlink" title="model_with_buckets"></a>model_with_buckets</h2><p>model_with_buckets函数用于创建支持buckets技术的seq2seq模型。所谓bucket可以理解成将输入按照不同长度指定到不同bucket中，例如将长度在15左右的输入串指定到长为20的bucket中；将长度在35左右的串指定到长为40的bucket中。指定bucket的好处是：（在tensorflow早期）dynamic_rnn还没有出现之前，所有的输入都需要被padding为完全相同的长度。假设数据集中只有一个特别冗长的句子，而其他句子都是短句，这种padding方式则显然不合适。比较折衷的办法是将长度比较接近的一部分句子padding为相同的长度。另一个原因是，神经网络通过一定阶段的训练，或许可以针对不同长度范围的文本产生合理的结果。</p>
<h4 id="输入参数-2"><a href="#输入参数-2" class="headerlink" title="输入参数"></a>输入参数</h4><p>1）<strong>encoder_inputs</strong>：A list of Tensors to feed the encoder; first seq2seq input. 一个传递到encoder的Tensor数组，编码器的输入；<br>2）<strong>decoder_inputs</strong>：A list of Tensors to feed the decoder; second seq2seq input. 一个传递到decoder的Tensor数组，解码器的输入；<br>3）<strong>targets</strong>：A list of 1D batch-sized int32 Tensors (desired output sequence). 一维batch大小的int32类型Tensor数组，期望的输出；<br>4）<strong>weights</strong>：List of 1D batch_sized float-Tensors to weight the targets. 一维batch大小的float类型Tensor数组，用于告知模型当前位置是否为<pad>，当前位置是原文本时weight=1，否则weight=0；<br>5）<strong>buckets</strong>：A list of pairs of (input size, output size) for each bucket. 一组(input_size，output_size)数组。用于标明每个bucket的输入和输出长度；<br>6）<strong>seq2seq</strong>：A sequence-to-sequence model function; it takes 2 input that agree with encoder_inputs and decoder_inputs, and returns a pair consisting of outputs and states (as e.g. basic_rnn_seq2seq). 传入上述的一种seq2seq模型函数；<br>7）<strong>softmax_loss_function=None</strong>：Function (label, logits) -&gt; loss-batch to be used instead of the standard softmax(the default if this is None). 默认采用标准softmax方法计算损失，在此处可定制。；<br>8）<strong>per_example_loss=False</strong>：If set, the returned loss will be a batch-sized tensor of losses for each sequence in the batch. If unset, it will be a scalar with the averaged loss from all examples. 若为False，返回当前batch平均损失；否则返回当前batch每个句子得到的损失；<br>9）<strong>name=None</strong>：Optional name for this operation, defaults to “model_with_buckets”. 设置返回模型的名称。</pad></p>
<h4 id="返回参数-2"><a href="#返回参数-2" class="headerlink" title="返回参数"></a>返回参数</h4><p>返回一个(outputs, losses)元组。其中：<br>1）<strong>outputs</strong>：The outputs of each bucket. Its j’th element consists of a list of 2D Tensors. The shape of output tensors can be either <strong>[batch_size x output_size]</strong> or <strong>[batch_size x num_decoder_symbols]</strong> depending on the seq2seq model used. outputs包含了每个bucket对应的输出。其中每个output Tensor的形状取决于对应的seq2seq模型。<br>2）<strong>losses</strong>：List of scalar Tensors, representing losses for each bucket, or, if per_example_loss is set, a list of 1D batch-sized float Tensors.一组实数Tensor，表示每个bucket对应的loss。如果设置了per_example_loss，则返回一个1维batch大小float类型的Tensor列表。</p>
<hr>
<h2 id="rnn-decoder"><a href="#rnn-decoder" class="headerlink" title="rnn_decoder"></a>rnn_decoder</h2><p>用于sequence-to-sequence模型的decoder</p>
<h4 id="输入参数-3"><a href="#输入参数-3" class="headerlink" title="输入参数"></a>输入参数</h4><p>1）<strong>decoder_inputs</strong>: A list of 2D Tensors [batch_size x input_size]. 一个2维tensor列表，作为decoder的输入；<br>2）<strong>initial_state</strong>: 2D Tensor with shape [batch_size x cell.state_size]. 一个2维Tensor，作为decoder的初始状态；<br>3）<strong>cell</strong>: core_rnn_cell.RNNCell defining the cell function and size. 定义了RNNCell的类型；<br>4）<strong>loop_function</strong>: If not None, this function will be applied to the i-th output in order to generate the i+1-st input, and decoder_inputs will be ignored, except for the first element (“GO” symbol). This can be used for decoding, but also for training to emulate <a href="http://arxiv.org/abs/1506.03099" target="_blank" rel="external">http://arxiv.org/abs/1506.03099</a>. Signature – loop_function(prev, i) = next<br>&emsp;&emsp;a. prev is a 2D Tensor of shape [batch_size x output_size],<br>&emsp;&emsp;b. i is an integer, the step number (when advanced control is needed),<br>&emsp;&emsp;c. next is a 2D Tensor of shape [batch_size x input_size].<br>&emsp;&emsp;如果loop_function不是None，就会将模型的上一时刻的输出作为下一时刻输入，并无视decoder_inputs输入；<br>5）<strong>scope</strong>: VariableScope for the created subgraph; defaults to “rnn_decoder”. 命名域，默认为”rnn_decoder”。</p>
<h4 id="返回参数-3"><a href="#返回参数-3" class="headerlink" title="返回参数"></a>返回参数</h4><p>返回一个(outputs, state)元组，其中：<br>1）<strong>outputs</strong>: A list of the same length as decoder_inputs of 2D Tensors with shape [batch_size x output_size] containing the generated outputs. 一组与decoder_inputs相同长度的2D-tensor。形状为[batch_size x output_size]<br>2）<strong>state</strong>: The state of each decoder cell in the final time-step. It is a 2D Tensor of shape [batch_size x cell.state_size]. 各个decoder在最后时间步的状态。形状为[batch_size, cell.state_size]</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="http://ruizhang1993.github.io/2017/05/01/Tensorflow之legacy-seq2seq模块初探-Tensorflow-r1-0-1/" data-id="cjgyvm8v9000jrgup76d55gfv" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/deep-learning/">deep learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/tensorflow/">tensorflow</a></li></ul>


    </footer>
  </div>
  
    
<ul id="article-nav" class="nav nav-pills nav-justified">
  
  <li role="presentation">
    <a href="/2017/04/21/CentOS-7-下-Cuda-Tensorflow-环境搭建实践/" id="article-nav-older" class="article-nav-link-wrap">
      <i class="fa fa-chevron-left pull-left"></i>
      <span class="article-nav-link-title">CentOS 7 下 Cuda &amp; Tensorflow 环境搭建实践</span>
    </a>
  </li>
  
  
  <li role="presentation">
    <a href="/2017/05/19/论文浅析-构建简单的神经网络对话生成模型/" id="article-nav-newer" class="article-nav-link-wrap">
      <span class="article-nav-link-title">构建简单的神经网络对话生成模型</span>
      <i class="fa fa-chevron-right pull-right"></i>
    </a>
  </li>
  
</ul>


  
</article>




        </div>
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
          
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <font size="4"><b>张睿</b></font></br> - 华南理工大学 博士生 </br> - 研究方向： </br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;自然语言处理、聊天机器人 </br> </br> <font size="4"><b>Rui Zhang</b></font></br> - Ph.D student @ SCUT </br> - Research domains: </br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NLP, chatbot

</div>


  


  
  <div class="sidebar-module">
    <h4>Tags</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/CNN/">CNN</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Lex/">Lex</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Mac/">Mac</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/NLP/">NLP</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/NLPCC-2018/">NLPCC 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/OPAD/">OPAD</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/OPADS/">OPADS</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/PLY/">PLY</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Python/">Python</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/SMPSA2017/">SMPSA2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Windows7/">Windows7</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/Yacc/">Yacc</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/cuda/">cuda</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/deep-learning/">deep learning</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/numpy/">numpy</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/python/">python</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/tensorflow/">tensorflow</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/xgboost/">xgboost</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/会议笔记/">会议笔记</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/数据挖掘/">数据挖掘</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/编译原理/">编译原理</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/tags/论文笔记/">论文笔记</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Tag Cloud</h4>
    <p class="tagcloud">
      <a href="/tags/CNN/" style="font-size: 10px;">CNN</a> <a href="/tags/Lex/" style="font-size: 10px;">Lex</a> <a href="/tags/Mac/" style="font-size: 10px;">Mac</a> <a href="/tags/NLP/" style="font-size: 17.5px;">NLP</a> <a href="/tags/NLPCC-2018/" style="font-size: 10px;">NLPCC 2018</a> <a href="/tags/OPAD/" style="font-size: 15px;">OPAD</a> <a href="/tags/OPADS/" style="font-size: 10px;">OPADS</a> <a href="/tags/PLY/" style="font-size: 10px;">PLY</a> <a href="/tags/Python/" style="font-size: 10px;">Python</a> <a href="/tags/SMPSA2017/" style="font-size: 10px;">SMPSA2017</a> <a href="/tags/Windows7/" style="font-size: 10px;">Windows7</a> <a href="/tags/Yacc/" style="font-size: 10px;">Yacc</a> <a href="/tags/cuda/" style="font-size: 10px;">cuda</a> <a href="/tags/deep-learning/" style="font-size: 20px;">deep learning</a> <a href="/tags/numpy/" style="font-size: 10px;">numpy</a> <a href="/tags/python/" style="font-size: 12.5px;">python</a> <a href="/tags/tensorflow/" style="font-size: 12.5px;">tensorflow</a> <a href="/tags/xgboost/" style="font-size: 10px;">xgboost</a> <a href="/tags/会议笔记/" style="font-size: 10px;">会议笔记</a> <a href="/tags/数据挖掘/" style="font-size: 10px;">数据挖掘</a> <a href="/tags/编译原理/" style="font-size: 10px;">编译原理</a> <a href="/tags/论文笔记/" style="font-size: 12.5px;">论文笔记</a>
    </p>
  </div>


  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/05/">May 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/04/">April 2018</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/06/">June 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/05/">May 2017</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/04/">April 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/01/">January 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/11/">November 2016</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/07/">July 2016</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/05/">May 2016</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/04/">April 2016</a><span class="sidebar-module-list-count">2</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2018/05/09/情绪敏感的人机交互/">情绪敏感的人机交互接口</a>
        </li>
      
        <li>
          <a href="/2018/04/28/开源语言理解和对话管理系统RASA/">开源语言理解和对话管理系统RASA</a>
        </li>
      
        <li>
          <a href="/2018/04/27/基于模板-Seq2Seq的大规模问句生成/">基于模板&amp;Seq2Seq的大规模问句生成</a>
        </li>
      
        <li>
          <a href="/2018/04/26/NLPCC2018评测Task4之规则匹配方法/">NLPCC2018评测Task4之规则匹配方法</a>
        </li>
      
        <li>
          <a href="/2018/04/23/OPAD-000-对话生成Related-Works/">对话生成Related Works（一）</a>
        </li>
      
    </ul>
  </div>



        </div>
    </div>
  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2018 RuiZhang1993<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

  

<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>



<script src="/js/script.js"></script>

</body>
</html>
